{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install feedparser\n",
    "#!pip install beautifulsoup4\n",
    "#!pip install bs4\n",
    "#!pip install lxml\n",
    "#!pip install pandas\n",
    "#!pip install umap\n",
    "#!pip install 'umap-learn==0.3.10'\n",
    "#!pip install requests\n",
    "#!pip install nltk\n",
    "#!pip install umap\n",
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append('venv/lib/python3.6/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "#import umap.umap_ as UMAP\n",
    "import getpass \n",
    "from bson.objectid import ObjectId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limpieza\n",
    "\n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = feedparser.parse('http://rss.cnn.com/rss/edition.rss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn['entries'][0]['media_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'title_detail', 'summary', 'summary_detail', 'links', 'link', 'id', 'guidislink', 'published', 'published_parsed', 'media_content'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn['entries'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.cnn.com/2019/12/13/asia/rohingya-suu-kyi-myanmar-hague-intl-hnk/index.html'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn['entries'][0]['link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The West turned Aung San Suu Kyi into a saint. She was always going to disappoint'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn['entries'][0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_parser():\n",
    "    CNN_list=[]\n",
    "    CNN_URLS = {\n",
    "    \"Top Stories\": \"http://rss.cnn.com/rss/edition.rss\",\n",
    "    \"World\": \"http://rss.cnn.com/rss/edition_world.rss\",\n",
    "    \"Africa\": \"http://rss.cnn.com/rss/edition_africa.rss\",\n",
    "    \"Americas\": \"http://rss.cnn.com/rss/edition_americas.rss\",\n",
    "    \"Asia\": \"http://rss.cnn.com/rss/edition_asia.rss\",\n",
    "    \"Europe\": \"http://rss.cnn.com/rss/edition_europe.rss\",\n",
    "    \"Middle East\": \"http://rss.cnn.com/rss/edition_meast.rss\",\n",
    "    \"U.S.\": \"http://rss.cnn.com/rss/edition_us.rss\",\n",
    "    \"Money\": \"http://rss.cnn.com/rss/money_news_international.rss\",\n",
    "    \"Technology\": \"http://rss.cnn.com/rss/edition_technology.rss\",\n",
    "    \"Science & Space\": \"http://rss.cnn.com/rss/edition_space.rss\",\n",
    "    \"Entertainment\":\"http://rss.cnn.com/rss/edition_entertainment.rss\",\n",
    "    \"World Sport\":\"http://rss.cnn.com/rss/edition_sport.rss\",\n",
    "    \"Football\" : \"http://rss.cnn.com/rss/edition_football.rss\",\n",
    "    \"Golf\": \"http://rss.cnn.com/rss/edition_golf.rss\",\n",
    "    \"Motorsport\": \"http://rss.cnn.com/rss/edition_motorsport.rss\",\n",
    "    \"Tennis\": \"http://rss.cnn.com/rss/edition_tennis.rss\",\n",
    "    \"Travel\":\"http://rss.cnn.com/rss/edition_travel.rss\",\n",
    "    \"Video\":\"http://rss.cnn.com/rss/cnn_freevideo.rss\",\n",
    "    \"Most Recent\":\"http://rss.cnn.com/rss/cnn_latest.rss\"}\n",
    "    for e in CNN_URLS:\n",
    "        #print('\\n La secci贸n es: '+e+ '\\n')\n",
    "        temp = feedparser.parse(CNN_URLS[e])\n",
    "        for i in range(len(temp['entries'])):\n",
    "            #print(temp['entries'][i]['title'])\n",
    "            CNN_list.append((str(datetime.datetime.now()),'Cable News Network',temp['entries'][i]['link'],temp['entries'][i]['title']))\n",
    "            \n",
    "        \n",
    "    #print(temp.keys)\n",
    "    return CNN_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_list = CNN_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FOX_parser():\n",
    "    FOX_list = []\n",
    "    FOX_URLS = {'Fox':\"https://www.foxnews.com/about/rss\"}\n",
    "    \n",
    "    \n",
    "    for e in FOX_URLS:\n",
    "        if hasattr(ssl, '_create_unverified_context'):\n",
    "            ssl._create_default_https_context = ssl._create_unverified_context\n",
    "        #print('\\n La secci贸n es: '+e+ '\\n')\n",
    "        temp = feedparser.parse(FOX_URLS[e])\n",
    "        for i in range(len(temp['entries'])):\n",
    "            #print(temp['entries'][i]['title'])\n",
    "            FOX_list.append((str(datetime.datetime.now()),'FOX News',temp['entries'][i]['link'],temp['entries'][i]['title']))\n",
    "            \n",
    "        \n",
    "    #print(temp.keys)\n",
    "    return FOX_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOX_list=FOX_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FOX_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def WSJ_parser():\n",
    "    WSJ_list=[]\n",
    "    WSJ_URLS = {\n",
    "    \"World\": \"https://feeds.a.dj.com/rss/RSSWorldNews.xml\",\n",
    "    \"Opinion\": \"https://feeds.a.dj.com/rss/RSSOpinion.xml\",\n",
    "    \"Business\":\"https://feeds.a.dj.com/rss/WSJcomUSBusiness.xml\",\n",
    "    \"Markets\":\"https://feeds.a.dj.com/rss/RSSMarketsMain.xml\",\n",
    "    \"Technology\":\"https://feeds.a.dj.com/rss/RSSWSJD.xml\",\n",
    "    \"Lifestyle\":\"https://feeds.a.dj.com/rss/RSSLifestyle.xml\"}\n",
    "    for e in WSJ_URLS:\n",
    "        #print('\\n La secci贸n es: '+e+ '\\n')\n",
    "        temp = feedparser.parse(WSJ_URLS[e])\n",
    "        for i in range(len(temp['entries'])):\n",
    "            #print(temp['entries'][i]['title'])\n",
    "            WSJ_list.append( (str(datetime.datetime.now()), 'The Wall Street Journal',temp['entries'][i]['link']  ,temp['entries'][i]['title']))\n",
    "            \n",
    "        \n",
    "    #print(temp.keys)\n",
    "    return WSJ_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ_list = WSJ_parser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BBC_parser():\n",
    "    BBC_list=[]\n",
    "    BBC_URLS = {\n",
    "    \"World\":\"http://feeds.bbci.co.uk/news/world/rss.xml\",\n",
    "    \"UK\":\"http://feeds.bbci.co.uk/news/uk/rss.xml\",\n",
    "    \"Business\":\"http://feeds.bbci.co.uk/news/business/rss.xml\",\n",
    "    \"Politics\": \"http://feeds.bbci.co.uk/news/politics/rss.xml\",\n",
    "    \"Health\": \"http://feeds.bbci.co.uk/news/health/rss.xml\",\n",
    "    \"Education\": \"http://feeds.bbci.co.uk/news/education/rss.xml\",\n",
    "    \"Science\": \"http://feeds.bbci.co.uk/news/science_and_environment/rss.xml\",\n",
    "    \"Technology\": \"http://feeds.bbci.co.uk/news/technology/rss.xml\",\n",
    "    \"Entretainment & Arts\": \"http://feeds.bbci.co.uk/news/entertainment_and_arts/rss.xml\"}\n",
    "    for e in BBC_URLS:\n",
    "        #print('\\n La secci贸n es: '+e+ '\\n')\n",
    "        temp = feedparser.parse(BBC_URLS[e])\n",
    "        for i in range(len(temp['entries'])):\n",
    "            #print(temp['entries'][i]['title'])\n",
    "            BBC_list.append(( str(datetime.datetime.now()), 'British Broadcasting Corporation',temp['entries'][i]['link'] ,temp['entries'][i]['title']))\n",
    "            \n",
    "        \n",
    "    #print(temp.keys)\n",
    "    return BBC_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_list = BBC_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def USAT_parser():\n",
    "    USAT_list=[]\n",
    "    USAT_URLS = {\n",
    "    \"US\":\"http://rssfeeds.usatoday.com/UsatodaycomNation-TopStories\",\n",
    "    \"World\":\"http://rssfeeds.usatoday.com/UsatodaycomWorld-TopStories\",\n",
    "    \"Opinion\":\"http://rssfeeds.usatoday.com/News-Opinion\",\n",
    "    \"Sports\":\"http://rssfeeds.usatoday.com/UsatodaycomSports-TopStories\",\n",
    "    \"Lifestyle\":\"http://rssfeeds.usatoday.com/usatoday-LifeTopStories\",\n",
    "    \"Money\":\"http://rssfeeds.usatoday.com/UsatodaycomMoney-TopStories\",\n",
    "    \"Tech\":\"http://rssfeeds.usatoday.com/usatoday-TechTopStories\",\n",
    "    \"Travel\":\"http://rssfeeds.usatoday.com/UsatodaycomTravel-TopStories\"}\n",
    "    for e in USAT_URLS:\n",
    "        #print('\\n La secci贸n es: '+e+ '\\n')\n",
    "        temp = feedparser.parse(USAT_URLS[e])\n",
    "        for i in range(len(temp['entries'])):\n",
    "            #print(temp['entries'][i]['title'])\n",
    "            USAT_list.append( ( str(datetime.datetime.now()),'USA Today' ,temp['entries'][i]['link'] ,temp['entries'][i]['title']))\n",
    "            \n",
    "        \n",
    "    #print(temp.keys)\n",
    "    return USAT_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "USAT_list = USAT_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NYT_parser():\n",
    "    NYT_list=[]\n",
    "    NYT_URLS = {\n",
    "    \"World\":\"https://rss.nytimes.com/services/xml/rss/nyt/World.xml\",\n",
    "    \"Politics\":\"https://rss.nytimes.com/services/xml/rss/nyt/Politics.xml\",\n",
    "    \"Business\":\"https://rss.nytimes.com/services/xml/rss/nyt/Business.xml\",\n",
    "    \"Technology\":\"https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml\",\n",
    "    \"Sports\":\"https://rss.nytimes.com/services/xml/rss/nyt/Sports.xml\"}\n",
    "    \n",
    "    for e in NYT_URLS:\n",
    "        #print('\\n La secci贸n es: '+e+ '\\n')\n",
    "        temp = feedparser.parse(NYT_URLS[e])\n",
    "        for i in range(len(temp['entries'])):\n",
    "            #print(temp['entries'][i]['title'])\n",
    "            NYT_list.append( (str(datetime.datetime.now()), 'New York Times',temp['entries'][i]['link'] ,temp['entries'][i]['title']))\n",
    "            \n",
    "        \n",
    "    #print(temp.keys)\n",
    "    return NYT_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_list =NYT_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "News = CNN_list + FOX_list+WSJ_list+BBC_list+USAT_list+NYT_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newsdf = pd.DataFrame(News, columns = ['date', 'News_fount' ,'link','Titles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg(s):\n",
    "    n = analyzer.polarity_scores(s)['neg']\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newsdf['Negative'] = Newsdf['Titles'].apply(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neut(s):\n",
    "    n = analyzer.polarity_scores(s)['neg']\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newsdf['Neutral'] = Newsdf['Titles'].apply(neut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(s):\n",
    "    n = analyzer.polarity_scores(s)['pos']\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newsdf['Positive'] = Newsdf['Titles'].apply(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def com(s):\n",
    "    n = analyzer.polarity_scores(s)['compound']\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newsdf['compound'] = Newsdf['Titles'].apply(com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Newsdf['token'] = Newsdf['Titles'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=English()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    tokens=parser(sentence)\n",
    "    \n",
    "    filtered_tokens=[]\n",
    "    for word in tokens:\n",
    "        lemma=word.lemma_.lower().strip()\n",
    "        #print (word, lemma)\n",
    "        if lemma not in STOP_WORDS and re.search('^[a-zA-Z]+$', lemma):\n",
    "            filtered_tokens.append(lemma)\n",
    "    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(tokenizer=spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix=tfidf.fit_transform(Newsdf['Titles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms=tfidf.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist=1-cosine_similarity(tfidf_matrix)\n",
    "#prueba = dist[0:98,0:98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan=HDBSCAN()\n",
    "clusters=hdbscan.fit_predict(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1335"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newsdf['clusters'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df=pd.DataFrame(tfidf_matrix.toarray(), columns=terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fast        7.770957\n",
       "facts       7.700801\n",
       "mahathir    1.146261\n",
       "mohamad     1.146261\n",
       "jintao      1.146261\n",
       "dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words_cluster=tfidf_df[clusters==3].T.sum(axis=1).sort_values(ascending=False)\n",
    "top_words_cluster.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_clustes = Newsdf.clusters.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "路路路路路路路路\n"
     ]
    }
   ],
   "source": [
    "p = getpass.getpass() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pymongo module\n",
    "import pymongo\n",
    "#import dns\n",
    "# connection string\n",
    "client = pymongo.MongoClient(\"mongodb+srv://uaqro:{}@cluster0-tmwuw.mongodb.net/test?retryWrites=true&w=majority\".format(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "db  =client.news_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_articles = db.articles\n",
    "col_fonts = db.fonts\n",
    "col_media = db.media\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivos = []\n",
    "for e in lab_clustes:\n",
    "    clustdf = Newsdf[Newsdf['clusters']==e]\n",
    "    words = tfidf_df[clusters==e].T.sum(axis=1).sort_values(ascending=False).index\n",
    "    #dictionew = {'headline':words[0].capitalize(), 'summary':words[0].capitalize()+' do '+words[1].capitalize(),\\\n",
    "                 #'ids':list(clustdf.id), 'link':list(clustdf.link)}\n",
    "    dictionew = {'cluster': str(e) ,'headline':words[0].capitalize(), 'summary':words[0].capitalize()+' do '+words[1].capitalize()}\n",
    "    archivos.append(dictionew)\n",
    "    #print(clustdf.link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "idclusts ={}\n",
    "for e in archivos:\n",
    "    documentos = e\n",
    "    #documentos['_id']=c\n",
    "    #print(documentos)\n",
    "    idennt = col_articles.insert_one(documentos).inserted_id\n",
    "    idclusts[str(e['cluster'])] = idennt\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "archi_fonts = []\n",
    "for e in lab_clustes:\n",
    "    clustdf = Newsdf[Newsdf['clusters']==e]\n",
    "    words = tfidf_df[clusters==e].T.sum(axis=1).sort_values(ascending=False).index\n",
    "    #dictionew = {'headline':words[0].capitalize(), 'summary':words[0].capitalize()+' do '+words[1].capitalize(),\\\n",
    "                 #'ids':list(clustdf.id), 'link':list(clustdf.link)}\n",
    "    dictionew = {'headline':words[0].capitalize(), 'subhead':words[0].capitalize()+' and '+words[1].capitalize()}\n",
    "    archi_fonts.append(dictionew)\n",
    "    #print(clustdf.link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_ids = []\n",
    "for i in range(len(Newsdf)):\n",
    "    #documentos = e\n",
    "    #documentos['_id']=c\n",
    "    #print(Newsdf.iloc[i]['Titles'])\n",
    "    idennt = col_fonts.insert_one({'headline':Newsdf.iloc[i]['Titles'], 'SubHead':Newsdf.iloc[i]['Titles']+' short',\\\n",
    "                                 'link': Newsdf.iloc[i]['link'] }).inserted_id\n",
    "    mongo_ids.append(idennt)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newsdf['_id'] = mongo_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "media = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "media['Name'] = ['Cable News Network', 'FOX News', 'The Wall Street Journal', 'British Broadcasting Corporation','USA Today', 'New York Times']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "media['logo'] = ['no lo tengo', 'no lo tengo', 'no lo tengo', 'no lo tengo', 'no lo tengo', 'http://nyt.com/logo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "media['web'] = ['https://www.cnn.com', 'https://www.foxnews.com', 'https://www.wsj.com', 'https://www.bbc.com', 'https://www.usatoday.com', 'https://www.nytimes.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "media['bias'] = ['驴Qu茅 es eso?','驴Qu茅 es eso?','驴Qu茅 es eso?','驴Qu茅 es eso?','驴Qu茅 es eso?','驴Qu茅 es eso?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "media['city'] = ['luego busco','luego busco','luego busco','luego busco','luego busco','luego busco']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "media['state'] = ['luego busco','luego busco','luego busco','luego busco','luego busco','luego busco']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "media['country'] = ['luego busco','luego busco','luego busco','luego busco','luego busco','luego busco']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "media['twitter'] = ['luego busco','luego busco','luego busco','luego busco','luego busco','luego busco']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "media['facebook'] = ['luego busco','luego busco','luego busco','luego busco','luego busco','luego busco']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "media['level'] = ['luego busco','luego busco','luego busco','luego busco','luego busco','luego busco']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_id = []\n",
    "for i in range(len(media)):\n",
    "    #documentos['_id']=c\n",
    "    #print(Newsdf.iloc[i]['Titles'])\n",
    "    idennt = col_media.insert_one({'name':media.iloc[i]['Name'], 'logo':media.iloc[i]['logo'],\\\n",
    "                                 'web': media.iloc[i]['web'], 'bias': media.iloc[i]['bias'], \\\n",
    "                                 'city': media.iloc[i]['city'], 'state': media.iloc[i]['state'], \\\n",
    "                                  'country': media.iloc[i]['country'], 'twitter': media.iloc[i]['twitter'], \\\n",
    "                                  'facebook': media.iloc[i]['facebook'], 'level': media.iloc[i]['level']}).inserted_id\n",
    "    news_id.append(idennt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "media['id'] = news_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'News_fount', 'link', 'Titles', 'Negative', 'Neutral',\n",
       "       'Positive', 'compound', 'clusters', '_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#col_articles.update_one({'cluster':'1'}, {'$set':{'prueba':'test'}})\n",
    "Newsdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in archivos:\n",
    "    tem = Newsdf[Newsdf['clusters'] == int(e['cluster'])]\n",
    "    col_articles.update_one({'cluster':e['cluster']}, {'$set':{'fonts':list(tem['_id'])}})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(media)):\n",
    "    #display(media.iloc[i]['id'])\n",
    "    #display(media.iloc[i]['Name'])\n",
    "    tem = Newsdf[Newsdf['News_fount'] == media.iloc[i]['Name']]\n",
    "    col_media.update_one({'name': media.iloc[i]['Name']}, {'$set':{'articles': list(tem['_id']) }})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###hasta ac谩 ya jala bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Newsdf)):\n",
    "    #display(media.iloc[i]['id'])\n",
    "    #display(media.iloc[i]['Name'])\n",
    "    tem = Newsdf.iloc[i]\n",
    "    medtem = media[media['Name'] == Newsdf.iloc[i]['News_fount']]\n",
    "    #print(tem)\n",
    "    col_fonts.update_one({'headline': Newsdf.iloc[i]['Titles']}, {'$set':{'media': medtem.iloc[0]['id'] }})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "medtem = media[media['Name'] == Newsdf.iloc[0]['News_fount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>logo</th>\n",
       "      <th>web</th>\n",
       "      <th>bias</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>twitter</th>\n",
       "      <th>facebook</th>\n",
       "      <th>level</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cable News Network</td>\n",
       "      <td>no lo tengo</td>\n",
       "      <td>https://www.cnn.com</td>\n",
       "      <td>驴Qu茅 es eso?</td>\n",
       "      <td>luego busco</td>\n",
       "      <td>luego busco</td>\n",
       "      <td>luego busco</td>\n",
       "      <td>luego busco</td>\n",
       "      <td>luego busco</td>\n",
       "      <td>luego busco</td>\n",
       "      <td>5df457b9c0e89026dc800978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Name         logo                  web          bias  \\\n",
       "0  Cable News Network  no lo tengo  https://www.cnn.com  驴Qu茅 es eso?   \n",
       "\n",
       "          city        state      country      twitter     facebook  \\\n",
       "0  luego busco  luego busco  luego busco  luego busco  luego busco   \n",
       "\n",
       "         level                        id  \n",
       "0  luego busco  5df457b9c0e89026dc800978  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documento = archivos[0]\n",
    "#idennt = colec.insert_one(documento).inserted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Newsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numb_clust = len(Newsdf.clusters.unique())\n",
    "numb_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'colec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-5601c651f860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"country\":\"USA\"}\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# insert document into collection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0midennt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocumentos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'colec' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "documentos = {\"_id\":75,\"company\":\"Capital One\",\n",
    "\"city\":\"McLean\",\n",
    "\"state\":\"VA\",\n",
    "\"country\":\"USA\"}\n",
    "# insert document into collection\n",
    "idennt = colec.insert_one(documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice = top_words_cluster.keys()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp(practice[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_words_cluster=tfidf_df[clusters==3].T.sum(axis=1).sort_values(ascending=False)\n",
    "top_words_cluster.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime module\n",
    "import datetime\n",
    "# import pymongo module\n",
    "import pymongo\n",
    "#import dns\n",
    "# connection string\n",
    "client = pymongo.MongoClient(\"mongodb+srv://uaqro:Croqueta1.@cluster0-tmwuw.mongodb.net/test?retryWrites=true&w=majority\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db  =client.news_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colec = db.news\n",
    "colec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos = {\"_id\":75,\"company\":\"Capital One\",\n",
    "\"city\":\"McLean\",\n",
    "\"state\":\"VA\",\n",
    "\"country\":\"USA\"}\n",
    "# insert document into collection\n",
    "idennt = colec.insert_one(documentos)\n",
    "idennt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = {\n",
    "  headline: \"Aqu铆 va el nuevo headline generado por nuestra hiper chingona IA\",\n",
    "  resume: \"Aqu铆 va el resumen que nos hemos inventado\",\n",
    "  fonts: [\"Aqu铆\", \"van\", \"cada\", \"uno\", \"de los ids de las art铆culos del cluster\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []\n",
    "adjectives = []\n",
    "for token in practice:\n",
    "    pass\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == 'ADJ':\n",
    "        adjectives.append(token)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"We try to explicitly describe the geometry of the edges of the images.\")\n",
    "\n",
    "for np in doc.noun_chunks: # use np instead of np.text\n",
    "    #print(np)\n",
    "    pass\n",
    "\n",
    "print()\n",
    "\n",
    "# code to recursively combine nouns\n",
    "# 'We' is actually a pronoun but included in your question\n",
    "# hence the token.pos_ == \"PRON\" part in the last if statement\n",
    "# suggest you extract PRON separately like the noun-chunks above\n",
    "\n",
    "index = 0\n",
    "nounIndices = []\n",
    "for token in doc:\n",
    "    # print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nounIndices.append(index)\n",
    "        print()\n",
    "    index = index + 1\n",
    "\n",
    "\n",
    "print(nounIndices)\n",
    "for idxValue in nounIndices:\n",
    "    doc = nlp(\"We try to explicitly describe the geometry of the edges of the images.\")\n",
    "    span = doc[doc[idxValue].left_edge.i : doc[idxValue].right_edge.i+1]\n",
    "    span.merge()\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'dobj' or token.dep_ == 'pobj' or token.pos_ == \"PRON\":\n",
    "            print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####De aqu铆 para abajo es el c贸digo de uaqro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_parser():\n",
    "    CNN_URLS = {\n",
    "    \"Top Stories\": \"http://rss.cnn.com/rss/edition.rss\",\n",
    "    \"World\": \"http://rss.cnn.com/rss/edition_world.rss\",\n",
    "    \"Africa\": \"http://rss.cnn.com/rss/edition_africa.rss\",\n",
    "    \"Americas\": \"http://rss.cnn.com/rss/edition_americas.rss\",\n",
    "    \"Asia\": \"http://rss.cnn.com/rss/edition_asia.rss\",\n",
    "    \"Europe\": \"http://rss.cnn.com/rss/edition_europe.rss\",\n",
    "    \"Middle East\": \"http://rss.cnn.com/rss/edition_meast.rss\",\n",
    "    \"U.S.\": \"http://rss.cnn.com/rss/edition_us.rss\",\n",
    "    \"Money\": \"http://rss.cnn.com/rss/money_news_international.rss\",\n",
    "    \"Technology\": \"http://rss.cnn.com/rss/edition_technology.rss\",\n",
    "    \"Science & Space\": \"http://rss.cnn.com/rss/edition_space.rss\",\n",
    "    \"Entertainment\":\"http://rss.cnn.com/rss/edition_entertainment.rss\",\n",
    "    \"World Sport\":\"http://rss.cnn.com/rss/edition_sport.rss\",\n",
    "    \"Football\" : \"http://rss.cnn.com/rss/edition_football.rss\",\n",
    "    \"Golf\": \"http://rss.cnn.com/rss/edition_golf.rss\",\n",
    "    \"Motorsport\": \"http://rss.cnn.com/rss/edition_motorsport.rss\",\n",
    "    \"Tennis\": \"http://rss.cnn.com/rss/edition_tennis.rss\",\n",
    "    \"Travel\":\"http://rss.cnn.com/rss/edition_travel.rss\",\n",
    "    \"Video\":\"http://rss.cnn.com/rss/cnn_freevideo.rss\",\n",
    "    \"Most Recent\":\"http://rss.cnn.com/rss/cnn_latest.rss\"}\n",
    "\n",
    "    def getSubtitle(i):\n",
    "        try: \n",
    "            return CNN_RSS.entries[i].summary_detail.value\n",
    "        except:\n",
    "            return \"N/A\"\n",
    "    def getPublished(i):\n",
    "        try: \n",
    "            return CNN_RSS.entries[i].published\n",
    "        except:\n",
    "            return \"N/A\"\n",
    "    def getMedia(i):\n",
    "        try: \n",
    "            return CNN_RSS.entries[i].media_content[0]['url']\n",
    "        except:\n",
    "            return \"N/A\"\n",
    "\n",
    "    CNN_feed = pd.DataFrame()\n",
    "    for k in range(len(CNN_URLS)):\n",
    "        CNN_RSS = feedparser.parse(list(CNN_URLS.values())[k])\n",
    "        for i in range(len(CNN_RSS.entries)):\n",
    "            title = [CNN_RSS.entries[i].title for i in range(len(CNN_RSS.entries))]\n",
    "            subtitle = [ getSubtitle(i) for i in range(len(CNN_RSS.entries))]\n",
    "            url = [CNN_RSS.entries[i].links[0].href for i in range(len(CNN_RSS.entries))]\n",
    "            timestamp = [getPublished(i) for i in range(len(CNN_RSS.entries))]\n",
    "            original_image = [getMedia(i) for i in range(len(CNN_RSS.entries))]\n",
    "            section = [list(CNN_URLS.keys())[k] for i in range(len(CNN_RSS.entries))]\n",
    "            CNN_RSS_feed_p = pd.DataFrame([title, subtitle, url, timestamp, original_image]).transpose()\n",
    "            CNN_feed.append(CNN_RSS_feed_p, ignore_index=True)\n",
    "    \n",
    "    return CNN_feed\n",
    "    \n",
    "def FOX_parser():\n",
    "    FOX_URLS = {'Fox':\"https://www.foxnews.com/about/rss\"}\n",
    "\n",
    "def WSJ_parser():\n",
    "    #Proxys?? Da error 503, hay que ver como hacerle\n",
    "    WSJ_URLS = {\n",
    "    \"World\": \"https://feeds.a.dj.com/rss/RSSWorldNews.xml\",\n",
    "    \"Opinion\": \"https://feeds.a.dj.com/rss/RSSOpinion.xml\",\n",
    "    \"Business\":\"https://feeds.a.dj.com/rss/WSJcomUSBusiness.xml\",\n",
    "    \"Markets\":\"https://feeds.a.dj.com/rss/RSSMarketsMain.xml\",\n",
    "    \"Technology\":\"https://feeds.a.dj.com/rss/RSSWSJD.xml\",\n",
    "    \"Lifestyle\":\"https://feeds.a.dj.com/rss/RSSLifestyle.xml\"}\n",
    "    \n",
    "def BBC_parser():\n",
    "    BBC_URLS = {\n",
    "    \"World\":\"http://feeds.bbci.co.uk/news/world/rss.xml\",\n",
    "    \"UK\":\"http://feeds.bbci.co.uk/news/uk/rss.xml\",\n",
    "    \"Business\":\"http://feeds.bbci.co.uk/news/business/rss.xml\",\n",
    "    \"Politics\": \"http://feeds.bbci.co.uk/news/politics/rss.xml\",\n",
    "    \"Health\": \"http://feeds.bbci.co.uk/news/health/rss.xml\",\n",
    "    \"Education\": \"http://feeds.bbci.co.uk/news/education/rss.xml\",\n",
    "    \"Science\": \"http://feeds.bbci.co.uk/news/science_and_environment/rss.xml\",\n",
    "    \"Technology\": \"http://feeds.bbci.co.uk/news/technology/rss.xml\",\n",
    "    \"Entretainment & Arts\": \"http://feeds.bbci.co.uk/news/entertainment_and_arts/rss.xml\"}\n",
    "\n",
    "    BBC_feed = pd.DataFrame()\n",
    "    for k in range(len(BBC_URLS)):\n",
    "        BBC_RSS = feedparser.parse(list(BBC_URLS.values())[k])\n",
    "        for i in range(len(BBC_RSS.entries)):\n",
    "            title = [BBC_RSS.entries[i].title for i in range(len(BBC_RSS.entries))]\n",
    "            subtitle = [ BBC_RSS.entries[i].summary for i in range(len(BBC_RSS.entries))]\n",
    "            url = [BBC_RSS.entries[i].links[0].href for i in range(len(BBC_RSS.entries))]\n",
    "            timestamp = [BBC_RSS.entries[0].published for i in range(len(BBC_RSS.entries))]\n",
    "            section = [list(BBC_RSS.keys())[k] for i in range(len(BBC_RSS.entries))]\n",
    "            BBC_RSS_feed_p = pd.DataFrame([title, subtitle, url, timestamp, section]).transpose()\n",
    "            BBC_feed.append(CNN_RSS_feed_p, ignore_index=True)\n",
    "    \n",
    "    return BBC_feed\n",
    "\n",
    "def USAT_parser():\n",
    "    \n",
    "    USAT_URLS = {\n",
    "    \"US\":\"http://rssfeeds.usatoday.com/UsatodaycomNation-TopStories\",\n",
    "    \"World\":\"http://rssfeeds.usatoday.com/UsatodaycomWorld-TopStories\",\n",
    "    \"Opinion\":\"http://rssfeeds.usatoday.com/News-Opinion\",\n",
    "    \"Sports\":\"http://rssfeeds.usatoday.com/UsatodaycomSports-TopStories\",\n",
    "    \"Lifestyle\":\"http://rssfeeds.usatoday.com/usatoday-LifeTopStories\",\n",
    "    \"Money\":\"http://rssfeeds.usatoday.com/UsatodaycomMoney-TopStories\",\n",
    "    \"Tech\":\"http://rssfeeds.usatoday.com/usatoday-TechTopStories\",\n",
    "    \"Travel\":\"http://rssfeeds.usatoday.com/UsatodaycomTravel-TopStories\"}\n",
    "    \n",
    "    def getUSATMedia(i):\n",
    "        try: \n",
    "            return USAT_feed_p.entries[i].links[1].href\n",
    "        except:\n",
    "            return \"N/A\"\n",
    "    USAT_feed = []\n",
    "    for k in range(len(USAT_URLS)):\n",
    "        USAT_RSS = feedparser.parse(list(USAT_URLS.values())[k]).entries\n",
    "        for i in range(len(USAT_RSS)):\n",
    "            title =  [USAT_feed_p.entries[i].title for i in range(len(USAT_RSS))]\n",
    "            #summary = [re.findall('(?<=<p>).*(?=</p>)', USAT_feed_p.entries[i].content[0].value)[0]) for i in range(len(USAT_RSS))]\n",
    "            url = [USAT_feed_p.entries[i].feedburner_origlink for i in range(len(USAT_RSS))]\n",
    "            timestamp = [USAT_feed_p.entries[i].published for i in range(len(USAT_RSS))]\n",
    "            image = [getUSATMedia(i) for i in range(len(USAT_RSS))]\n",
    "            section = [list(USAT_URLS.keys())[k] for i in range(len(USAT_RSS))]\n",
    "            USAT_feed.append([title, summary, url, timestamp, image, section])\n",
    "    \n",
    "    return USAT_feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_RSS = feedparser.parse(\"https://rss.nytimes.com/services/xml/rss/nyt/World.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_RSS.entries[0].media_content[0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_RSS.entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_feed = []\n",
    "def getNYTMedia(i):\n",
    "    try: \n",
    "        return media == NYT_RSS.entries[i].media_content[0]['url']\n",
    "    except:\n",
    "        return \"N/A\"\n",
    "    \n",
    "for k in range(len(NYT_URLS)):\n",
    "    NYT_RSS = feedparser.parse(list(NYT_URLS.values())[k]).entries\n",
    "    for i in range(len(NYT_RSS)):\n",
    "        title = [NYT_RSS.entries[i].title for i in range(len(NYT_RSS))]\n",
    "        url = [NYT_RSS.entries[i].link for i in range(len(NYT_RSS))]\n",
    "        summary = [NYT_RSS.entries[i].summary for i in range(len(NYT_RSS))]\n",
    "        timestamp = [NYT_RSS.entries[i].published for i in range(len(NYT_RSS))]\n",
    "        media = [getNYTMedia(i) for i in range(len(NYT_RSS))]\n",
    "        section = [list(NYT_URLS.keys())[k] for i in range(len(NYT_RSS))]\n",
    "        NYT_feed.append([title, summary, url, timestamp, media, section])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_URLS = {\n",
    "    \"World\":\"https://rss.nytimes.com/services/xml/rss/nyt/World.xml\",\n",
    "    \"Africa\":\n",
    "    \"Americas\":\n",
    "    \"Asia Pacific\":\n",
    "    \"Europe\":\n",
    "    \"Middle East\":\n",
    "    \"US\":\n",
    "    \"Education\":\n",
    "    \"Politics\":\"https://rss.nytimes.com/services/xml/rss/nyt/Politics.xml\"\n",
    "    \"Business\":\"https://rss.nytimes.com/services/xml/rss/nyt/Business.xml\",\n",
    "    \"Technology\":\"https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml\",\n",
    "    \"Sports\":\"https://rss.nytimes.com/services/xml/rss/nyt/Sports.xml\",\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
