{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First of all, I import all of the libreries that I will use.\n",
    "import datetime\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "#import umap.umap_ as UMAP\n",
    "import getpass \n",
    "from bson.objectid import ObjectId\n",
    "# import pymongo module\n",
    "import pymongo\n",
    "import nltk\n",
    "\n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "import re\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, I define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_parser():\n",
    "    CNN_list=[]\n",
    "    CNN_URLS = {\n",
    "    #\"Top Stories\": \"http://rss.cnn.com/rss/edition.rss\",\n",
    "    \"World\": \"http://rss.cnn.com/rss/edition_world.rss\",\n",
    "    \"Africa\": \"http://rss.cnn.com/rss/edition_africa.rss\",\n",
    "    \"Americas\": \"http://rss.cnn.com/rss/edition_americas.rss\",\n",
    "    \"Asia\": \"http://rss.cnn.com/rss/edition_asia.rss\",\n",
    "    \"Europe\": \"http://rss.cnn.com/rss/edition_europe.rss\",\n",
    "    \"Middle East\": \"http://rss.cnn.com/rss/edition_meast.rss\",\n",
    "    \"U.S.\": \"http://rss.cnn.com/rss/edition_us.rss\",\n",
    "    \"Money\": \"http://rss.cnn.com/rss/money_news_international.rss\",\n",
    "    \"Technology\": \"http://rss.cnn.com/rss/edition_technology.rss\",\n",
    "    \"Science & Space\": \"http://rss.cnn.com/rss/edition_space.rss\",\n",
    "    \"Entertainment\":\"http://rss.cnn.com/rss/edition_entertainment.rss\",\n",
    "    \"World Sport\":\"http://rss.cnn.com/rss/edition_sport.rss\",\n",
    "    \"Football\" : \"http://rss.cnn.com/rss/edition_football.rss\",\n",
    "    \"Golf\": \"http://rss.cnn.com/rss/edition_golf.rss\",\n",
    "    \"Motorsport\": \"http://rss.cnn.com/rss/edition_motorsport.rss\",\n",
    "    \"Tennis\": \"http://rss.cnn.com/rss/edition_tennis.rss\",\n",
    "    \"Travel\":\"http://rss.cnn.com/rss/edition_travel.rss\",\n",
    "    \"Video\":\"http://rss.cnn.com/rss/cnn_freevideo.rss\",\n",
    "    \"Most Recent\":\"http://rss.cnn.com/rss/cnn_latest.rss\"}\n",
    "    for e in CNN_URLS:\n",
    "        #print('\\n La secci贸n es: '+e+ '\\n')\n",
    "        temp = feedparser.parse(CNN_URLS[e])\n",
    "        for i in range(len(temp['entries'])):\n",
    "            #print(temp['entries'][i]['title'])\n",
    "            CNN_list.append((str(datetime.datetime.now()),'Cable News Network',temp['entries'][i]['link'],temp['entries'][i]['title'], temp['entries'][i]['summary']))\n",
    "            \n",
    "        \n",
    "    #print(temp.keys)\n",
    "    return CNN_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FOX_parser():\n",
    "    FOX_list = []\n",
    "    FOX_URLS = {'Fox':\"https://www.foxnews.com/about/rss\"}\n",
    "    \n",
    "    \n",
    "    for e in FOX_URLS:\n",
    "        if hasattr(ssl, '_create_unverified_context'):\n",
    "            ssl._create_default_https_context = ssl._create_unverified_context\n",
    "        #print('\\n La secci贸n es: '+e+ '\\n')\n",
    "        temp = feedparser.parse(FOX_URLS[e])\n",
    "        for i in range(len(temp['entries'])):\n",
    "            #print(temp['entries'][i]['title'])\n",
    "            FOX_list.append((str(datetime.datetime.now()),'FOX News',temp['entries'][i]['link'],temp['entries'][i]['title']), temp['entries'][i]['summary'])\n",
    "            \n",
    "        \n",
    "    #print(temp.keys)\n",
    "    return FOX_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def WSJ_parser():\n",
    "    WSJ_list=[]\n",
    "    WSJ_URLS = {\n",
    "    \"World\": \"https://feeds.a.dj.com/rss/RSSWorldNews.xml\",\n",
    "    \"Opinion\": \"https://feeds.a.dj.com/rss/RSSOpinion.xml\",\n",
    "    \"Business\":\"https://feeds.a.dj.com/rss/WSJcomUSBusiness.xml\",\n",
    "    \"Markets\":\"https://feeds.a.dj.com/rss/RSSMarketsMain.xml\",\n",
    "    \"Technology\":\"https://feeds.a.dj.com/rss/RSSWSJD.xml\",\n",
    "    \"Lifestyle\":\"https://feeds.a.dj.com/rss/RSSLifestyle.xml\"}\n",
    "    for e in WSJ_URLS:\n",
    "        #print('\\n La secci贸n es: '+e+ '\\n')\n",
    "        temp = feedparser.parse(WSJ_URLS[e])\n",
    "        for i in range(len(temp['entries'])):\n",
    "            #print(temp['entries'][i]['title'])\n",
    "            WSJ_list.append( (str(datetime.datetime.now()), 'The Wall Street Journal',temp['entries'][i]['link']  ,temp['entries'][i]['title'], temp['entries'][i]['summary']))\n",
    "            \n",
    "        \n",
    "    #print(temp.keys)\n",
    "    return WSJ_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BBC_parser():\n",
    "    BBC_list=[]\n",
    "    BBC_URLS = {\n",
    "    \"World\":\"http://feeds.bbci.co.uk/news/world/rss.xml\",\n",
    "    \"UK\":\"http://feeds.bbci.co.uk/news/uk/rss.xml\",\n",
    "    \"Business\":\"http://feeds.bbci.co.uk/news/business/rss.xml\",\n",
    "    \"Politics\": \"http://feeds.bbci.co.uk/news/politics/rss.xml\",\n",
    "    \"Health\": \"http://feeds.bbci.co.uk/news/health/rss.xml\",\n",
    "    \"Education\": \"http://feeds.bbci.co.uk/news/education/rss.xml\",\n",
    "    \"Science\": \"http://feeds.bbci.co.uk/news/science_and_environment/rss.xml\",\n",
    "    \"Technology\": \"http://feeds.bbci.co.uk/news/technology/rss.xml\",\n",
    "    \"Entretainment & Arts\": \"http://feeds.bbci.co.uk/news/entertainment_and_arts/rss.xml\"}\n",
    "    for e in BBC_URLS:\n",
    "        #print('\\n La secci贸n es: '+e+ '\\n')\n",
    "        temp = feedparser.parse(BBC_URLS[e])\n",
    "        for i in range(len(temp['entries'])):\n",
    "            #print(temp['entries'][i]['title'])\n",
    "            BBC_list.append(( str(datetime.datetime.now()), 'British Broadcasting Corporation',temp['entries'][i]['link'] ,temp['entries'][i]['title'], temp['entries'][i]['summary']))\n",
    "            \n",
    "        \n",
    "    #print(temp.keys)\n",
    "    return BBC_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def USAT_parser():\n",
    "    USAT_list=[]\n",
    "    USAT_URLS = {\n",
    "    \"US\":\"http://rssfeeds.usatoday.com/UsatodaycomNation-TopStories\",\n",
    "    \"World\":\"http://rssfeeds.usatoday.com/UsatodaycomWorld-TopStories\",\n",
    "    \"Opinion\":\"http://rssfeeds.usatoday.com/News-Opinion\",\n",
    "    \"Sports\":\"http://rssfeeds.usatoday.com/UsatodaycomSports-TopStories\",\n",
    "    \"Lifestyle\":\"http://rssfeeds.usatoday.com/usatoday-LifeTopStories\",\n",
    "    \"Money\":\"http://rssfeeds.usatoday.com/UsatodaycomMoney-TopStories\",\n",
    "    \"Tech\":\"http://rssfeeds.usatoday.com/usatoday-TechTopStories\",\n",
    "    \"Travel\":\"http://rssfeeds.usatoday.com/UsatodaycomTravel-TopStories\"}\n",
    "    for e in USAT_URLS:\n",
    "        #print('\\n La secci贸n es: '+e+ '\\n')\n",
    "        temp = feedparser.parse(USAT_URLS[e])\n",
    "        for i in range(len(temp['entries'])):\n",
    "            #print(temp['entries'][i]['title'])\n",
    "            USAT_list.append( ( str(datetime.datetime.now()),'USA Today' ,temp['entries'][i]['link'] ,temp['entries'][i]['title'], temp['entries'][i]['summary']))\n",
    "            \n",
    "        \n",
    "    #print(temp.keys)\n",
    "    return USAT_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NYT_parser():\n",
    "    NYT_list=[]\n",
    "    NYT_URLS = {\n",
    "    \"World\":\"https://rss.nytimes.com/services/xml/rss/nyt/World.xml\",\n",
    "    \"Politics\":\"https://rss.nytimes.com/services/xml/rss/nyt/Politics.xml\",\n",
    "    \"Business\":\"https://rss.nytimes.com/services/xml/rss/nyt/Business.xml\",\n",
    "    \"Technology\":\"https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml\",\n",
    "    \"Sports\":\"https://rss.nytimes.com/services/xml/rss/nyt/Sports.xml\"}\n",
    "    \n",
    "    for e in NYT_URLS:\n",
    "        #print('\\n La secci贸n es: '+e+ '\\n')\n",
    "        temp = feedparser.parse(NYT_URLS[e])\n",
    "        for i in range(len(temp['entries'])):\n",
    "            #print(temp['entries'][i]['title'])\n",
    "            NYT_list.append( (str(datetime.datetime.now()), 'New York Times',temp['entries'][i]['link'] ,temp['entries'][i]['title'], temp['entries'][i]['summary']))\n",
    "            \n",
    "        \n",
    "    #print(temp.keys)\n",
    "    return NYT_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_fun(s, n_n = 7, lsen = 500):\n",
    "    if s == '': return ''\n",
    "    # Removing Square Brackets and Extra Spaces\n",
    "    article_text = re.sub(r'\\[[0-9]*\\]', ' ', s)\n",
    "    article_text = re.sub(r'\\s+', ' ', article_text)\n",
    "    # Removing special characters and digits\n",
    "    formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n",
    "    formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "    #Tokenize the text in sentences\n",
    "    sentence_list = nltk.sent_tokenize(article_text)\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    word_frequencies = {}\n",
    "    for word in nltk.word_tokenize(formatted_article_text):\n",
    "        if word not in stopwords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "        \n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < lsen and len(sent.split(' ')) > 1:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "    \n",
    "    summary_sentences = heapq.nlargest(n_n, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "    fin_summary = ' '.join(summary_sentences)\n",
    "    return fin_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next step is to scrap the news of the day and save them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_list = CNN_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOX_list=FOX_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ_list = WSJ_parser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_list = BBC_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "USAT_list = USAT_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_list =NYT_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "News = CNN_list + FOX_list+WSJ_list+BBC_list+USAT_list+NYT_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now convert the list to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newsdf = pd.DataFrame(News, columns = ['date', 'News_fount' ,'link','Titles', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The sumaries are a little dirty, so let's clean them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sum(s):\n",
    "    #s = s\n",
    "    s = re.sub('<.{1,}>', ' ', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newsdf['Summary'] = Newsdf['Summary'].apply(clean_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this step, we make a sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg(s):\n",
    "    n = analyzer.polarity_scores(s)['neg']\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neut(s):\n",
    "    n = analyzer.polarity_scores(s)['neu']\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(s):\n",
    "    n = analyzer.polarity_scores(s)['pos']\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def com(s):\n",
    "    n = analyzer.polarity_scores(s)['compound']\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newsdf['Negative'] = Newsdf['Summary'].apply(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newsdf['Neutral'] = Newsdf['Summary'].apply(neut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newsdf['Positive'] = Newsdf['Summary'].apply(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newsdf['compound'] = Newsdf['Summary'].apply(com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, I clusterize only taking into account the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=English()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    tokens=parser(sentence)\n",
    "    \n",
    "    filtered_tokens=[]\n",
    "    for word in tokens:\n",
    "        lemma=word.lemma_.lower().strip()\n",
    "        #print (word, lemma)\n",
    "        if lemma not in STOP_WORDS and re.search('^[a-zA-Z]+$', lemma):\n",
    "            filtered_tokens.append(lemma)\n",
    "    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(tokenizer=spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix=tfidf.fit_transform(Newsdf['Titles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms=tfidf.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist=1-cosine_similarity(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan=HDBSCAN()\n",
    "clusters=hdbscan.fit_predict(dist)\n",
    "#other possible clusterizers\n",
    "#dbscan = DBSCAN()\n",
    "#kmeans = KMeans()\n",
    "#clusters = kmeans.fit_predict(dist)\n",
    "#clusters=dbscan.fit_predict(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newsdf['clusters'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this section, I create a dictionary to save statistics for every cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterdf(n_c):\n",
    "    cluster =Newsdf[Newsdf['clusters']== n_c]\n",
    "    sent = cluster.groupby('News_fount').mean()\n",
    "    sent = sent[['compound','Negative', 'Neutral', 'Positive']]\n",
    "    hist = '. '.join(cluster['Titles']) + ' ' + ' '.join(cluster['Summary'])\n",
    "    return {'title':summary_fun(hist,1, 20),'sumary':summary_fun(hist), 'sent':sent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust = {}\n",
    "for i in set(clusters):\n",
    "    #clust.append(clusterdf(i))\n",
    "    clust['cluster {}'.format(i)] = clusterdf(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '',\n",
       " 'sumary': \"Featured Hidden Common Ground Nation's failed weed war Wedding party etiquette Snacks that ruin diets Ranking top airlines Most walkable U.S. cities\",\n",
       " 'sent':             compound  Negative   Neutral  Positive\n",
       " News_fount                                        \n",
       " USA Today  -0.130617  0.223667  0.601667  0.174667}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust['cluster 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf_df=pd.DataFrame(tfidf_matrix.toarray(), columns=terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_words_cluster=tfidf_df[clusters==0].T.sum(axis=1).sort_values(ascending=False)\n",
    "#top_words_cluster.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_clustes = Newsdf.clusters.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we upload the data to mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "路路路路路路路路\n"
     ]
    }
   ],
   "source": [
    "p = getpass.getpass() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import dns\n",
    "# connection string\n",
    "client = pymongo.MongoClient(\"mongodb+srv://uaqro:{}@cluster0-tmwuw.mongodb.net/test?retryWrites=true&w=majority\".format(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "db  =client.news_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_articles = db.articles\n",
    "col_fonts = db.fonts\n",
    "col_media = db.media\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Get the news and analysis to help you plan well delivered to your inbox. Newsletter Parenting trends, news The Daily Money  Retiring is hard work! Talking Tech Latest Travel news'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust['cluster 1']['sumary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivos = []\n",
    "for e in lab_clustes:\n",
    "    clustdf = Newsdf[Newsdf['clusters']==e]\n",
    "    #words = tfidf_df[clusters==e].T.sum(axis=1).sort_values(ascending=False).index\n",
    "    #dictionew = {'headline':words[0].capitalize(), 'summary':words[0].capitalize()+' do '+words[1].capitalize(),\\\n",
    "                 #'ids':list(clustdf.id), 'link':list(clustdf.link)}\n",
    "    #dictionew = {'cluster': str(e) ,'headline':words[0].capitalize(), 'summary':words[0].capitalize()+' do '+words[1].capitalize()}\n",
    "    dictionew = {'cluster': str(e) ,'headline':clust['cluster {}'.format(e)]['title'], 'summary':clust['cluster {}'.format(e)]['sumary']}\n",
    "    archivos.append(dictionew)\n",
    "    #print(clustdf.link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "idclusts ={}\n",
    "for e in archivos:\n",
    "    documentos = e\n",
    "    #documentos['_id']=c\n",
    "    #print(documentos)\n",
    "    idennt = col_articles.insert_one(documentos).inserted_id\n",
    "    idclusts[str(e['cluster'])] = idennt\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "archi_fonts = []\n",
    "for i in range(len(Newsdf)):\n",
    "    #print(Newsdf['Titles'][i])\n",
    "    dictionew = {'headline':Newsdf['Titles'][i], 'subhead':Newsdf['Summary'][i], 'link':Newsdf['link'][i]}\n",
    "    archi_fonts.append(dictionew)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#archi_fonts = []\n",
    "#for e in lab_clustes:\n",
    "#    clustdf = Newsdf[Newsdf['clusters']==e]\n",
    "    #words = tfidf_df[clusters==e].T.sum(axis=1).sort_values(ascending=False).index\n",
    "    #dictionew = {'headline':words[0].capitalize(), 'summary':words[0].capitalize()+' do '+words[1].capitalize(),\\\n",
    "                 #'ids':list(clustdf.id), 'link':list(clustdf.link)}\n",
    "    #dictionew = {'headline':words[0].capitalize(), 'subhead':words[0].capitalize()+' and '+words[1].capitalize()}\n",
    "    #hist = '. '.join(clustdf['Titles']) + ' ' + ' '.join(clustdf['Summary'])\n",
    "#    dictionew = {'headline':'cluster {}'.format(e), 'subhead':words[0].capitalize()+' and '+words[1].capitalize()}\n",
    "#    archi_fonts.append(dictionew)\n",
    "    #print(clustdf.link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_ids = []\n",
    "for e in archi_fonts:\n",
    "    #documentos = e\n",
    "    #documentos['_id']=c\n",
    "    #print(Newsdf.iloc[i]['Titles'])\n",
    "    idennt = col_fonts.insert_one(e).inserted_id\n",
    "    mongo_ids.append(idennt)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newsdf['_id'] = mongo_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "media = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "media['Name'] = ['Cable News Network', 'FOX News', 'The Wall Street Journal', 'British Broadcasting Corporation','USA Today', 'New York Times']\n",
    "media['logo'] = ['no lo tengo', 'no lo tengo', 'no lo tengo', 'no lo tengo', 'no lo tengo', 'http://nyt.com/logo']\n",
    "media['web'] = ['https://www.cnn.com', 'https://www.foxnews.com', 'https://www.wsj.com', 'https://www.bbc.com', 'https://www.usatoday.com', 'https://www.nytimes.com']\n",
    "media['bias'] = ['驴Qu茅 es eso?','驴Qu茅 es eso?','驴Qu茅 es eso?','驴Qu茅 es eso?','驴Qu茅 es eso?','驴Qu茅 es eso?']\n",
    "media['city'] = ['luego busco','luego busco','luego busco','luego busco','luego busco','luego busco']\n",
    "media['state'] = ['luego busco','luego busco','luego busco','luego busco','luego busco','luego busco']\n",
    "media['country'] = ['luego busco','luego busco','luego busco','luego busco','luego busco','luego busco']\n",
    "media['twitter'] = ['luego busco','luego busco','luego busco','luego busco','luego busco','luego busco']\n",
    "media['facebook'] = ['luego busco','luego busco','luego busco','luego busco','luego busco','luego busco']\n",
    "media['level'] = ['luego busco','luego busco','luego busco','luego busco','luego busco','luego busco']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_id = []\n",
    "for i in range(len(media)):\n",
    "    #documentos['_id']=c\n",
    "    #print(Newsdf.iloc[i]['Titles'])\n",
    "    idennt = col_media.insert_one({'name':media.iloc[i]['Name'], 'logo':media.iloc[i]['logo'],\\\n",
    "                                 'web': media.iloc[i]['web'], 'bias': media.iloc[i]['bias'], \\\n",
    "                                 'city': media.iloc[i]['city'], 'state': media.iloc[i]['state'], \\\n",
    "                                  'country': media.iloc[i]['country'], 'twitter': media.iloc[i]['twitter'], \\\n",
    "                                  'facebook': media.iloc[i]['facebook'], 'level': media.iloc[i]['level']}).inserted_id\n",
    "    news_id.append(idennt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "media['id'] = news_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in archivos:\n",
    "    tem = Newsdf[Newsdf['clusters'] == int(e['cluster'])]\n",
    "    col_articles.update_one({'cluster':e['cluster']}, {'$set':{'fonts':list(tem['_id'])}})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(media)):\n",
    "    #display(media.iloc[i]['id'])\n",
    "    #display(media.iloc[i]['Name'])\n",
    "    tem = Newsdf[Newsdf['News_fount'] == media.iloc[i]['Name']]\n",
    "    col_media.update_one({'name': media.iloc[i]['Name']}, {'$set':{'articles': list(tem['_id']) }})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Newsdf)):\n",
    "    #display(media.iloc[i]['id'])\n",
    "    #display(media.iloc[i]['Name'])\n",
    "    tem = Newsdf.iloc[i]\n",
    "    medtem = media[media['Name'] == Newsdf.iloc[i]['News_fount']]\n",
    "    #print(tem)\n",
    "    col_fonts.update_one({'headline': Newsdf.iloc[i]['Titles']}, {'$set':{'media': medtem.iloc[0]['id'] }})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#medtem = media[media['Name'] == Newsdf.iloc[0]['News_fount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documento = archivos[0]\n",
    "#idennt = colec.insert_one(documento).inserted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Newsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numb_clust = len(Newsdf.clusters.unique())\n",
    "numb_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'colec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-5601c651f860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"country\":\"USA\"}\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# insert document into collection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0midennt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocumentos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'colec' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "documentos = {\"_id\":75,\"company\":\"Capital One\",\n",
    "\"city\":\"McLean\",\n",
    "\"state\":\"VA\",\n",
    "\"country\":\"USA\"}\n",
    "# insert document into collection\n",
    "idennt = colec.insert_one(documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice = top_words_cluster.keys()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp(practice[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_words_cluster=tfidf_df[clusters==3].T.sum(axis=1).sort_values(ascending=False)\n",
    "top_words_cluster.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime module\n",
    "import datetime\n",
    "# import pymongo module\n",
    "import pymongo\n",
    "#import dns\n",
    "# connection string\n",
    "client = pymongo.MongoClient(\"mongodb+srv://uaqro:Croqueta1.@cluster0-tmwuw.mongodb.net/test?retryWrites=true&w=majority\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db  =client.news_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colec = db.news\n",
    "colec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos = {\"_id\":75,\"company\":\"Capital One\",\n",
    "\"city\":\"McLean\",\n",
    "\"state\":\"VA\",\n",
    "\"country\":\"USA\"}\n",
    "# insert document into collection\n",
    "idennt = colec.insert_one(documentos)\n",
    "idennt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = {\n",
    "  headline: \"Aqu铆 va el nuevo headline generado por nuestra hiper chingona IA\",\n",
    "  resume: \"Aqu铆 va el resumen que nos hemos inventado\",\n",
    "  fonts: [\"Aqu铆\", \"van\", \"cada\", \"uno\", \"de los ids de las art铆culos del cluster\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []\n",
    "adjectives = []\n",
    "for token in practice:\n",
    "    pass\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == 'ADJ':\n",
    "        adjectives.append(token)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"We try to explicitly describe the geometry of the edges of the images.\")\n",
    "\n",
    "for np in doc.noun_chunks: # use np instead of np.text\n",
    "    #print(np)\n",
    "    pass\n",
    "\n",
    "print()\n",
    "\n",
    "# code to recursively combine nouns\n",
    "# 'We' is actually a pronoun but included in your question\n",
    "# hence the token.pos_ == \"PRON\" part in the last if statement\n",
    "# suggest you extract PRON separately like the noun-chunks above\n",
    "\n",
    "index = 0\n",
    "nounIndices = []\n",
    "for token in doc:\n",
    "    # print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nounIndices.append(index)\n",
    "        print()\n",
    "    index = index + 1\n",
    "\n",
    "\n",
    "print(nounIndices)\n",
    "for idxValue in nounIndices:\n",
    "    doc = nlp(\"We try to explicitly describe the geometry of the edges of the images.\")\n",
    "    span = doc[doc[idxValue].left_edge.i : doc[idxValue].right_edge.i+1]\n",
    "    span.merge()\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'dobj' or token.dep_ == 'pobj' or token.pos_ == \"PRON\":\n",
    "            print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####De aqu铆 para abajo es el c贸digo de uaqro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_parser():\n",
    "    CNN_URLS = {\n",
    "    \"Top Stories\": \"http://rss.cnn.com/rss/edition.rss\",\n",
    "    \"World\": \"http://rss.cnn.com/rss/edition_world.rss\",\n",
    "    \"Africa\": \"http://rss.cnn.com/rss/edition_africa.rss\",\n",
    "    \"Americas\": \"http://rss.cnn.com/rss/edition_americas.rss\",\n",
    "    \"Asia\": \"http://rss.cnn.com/rss/edition_asia.rss\",\n",
    "    \"Europe\": \"http://rss.cnn.com/rss/edition_europe.rss\",\n",
    "    \"Middle East\": \"http://rss.cnn.com/rss/edition_meast.rss\",\n",
    "    \"U.S.\": \"http://rss.cnn.com/rss/edition_us.rss\",\n",
    "    \"Money\": \"http://rss.cnn.com/rss/money_news_international.rss\",\n",
    "    \"Technology\": \"http://rss.cnn.com/rss/edition_technology.rss\",\n",
    "    \"Science & Space\": \"http://rss.cnn.com/rss/edition_space.rss\",\n",
    "    \"Entertainment\":\"http://rss.cnn.com/rss/edition_entertainment.rss\",\n",
    "    \"World Sport\":\"http://rss.cnn.com/rss/edition_sport.rss\",\n",
    "    \"Football\" : \"http://rss.cnn.com/rss/edition_football.rss\",\n",
    "    \"Golf\": \"http://rss.cnn.com/rss/edition_golf.rss\",\n",
    "    \"Motorsport\": \"http://rss.cnn.com/rss/edition_motorsport.rss\",\n",
    "    \"Tennis\": \"http://rss.cnn.com/rss/edition_tennis.rss\",\n",
    "    \"Travel\":\"http://rss.cnn.com/rss/edition_travel.rss\",\n",
    "    \"Video\":\"http://rss.cnn.com/rss/cnn_freevideo.rss\",\n",
    "    \"Most Recent\":\"http://rss.cnn.com/rss/cnn_latest.rss\"}\n",
    "\n",
    "    def getSubtitle(i):\n",
    "        try: \n",
    "            return CNN_RSS.entries[i].summary_detail.value\n",
    "        except:\n",
    "            return \"N/A\"\n",
    "    def getPublished(i):\n",
    "        try: \n",
    "            return CNN_RSS.entries[i].published\n",
    "        except:\n",
    "            return \"N/A\"\n",
    "    def getMedia(i):\n",
    "        try: \n",
    "            return CNN_RSS.entries[i].media_content[0]['url']\n",
    "        except:\n",
    "            return \"N/A\"\n",
    "\n",
    "    CNN_feed = pd.DataFrame()\n",
    "    for k in range(len(CNN_URLS)):\n",
    "        CNN_RSS = feedparser.parse(list(CNN_URLS.values())[k])\n",
    "        for i in range(len(CNN_RSS.entries)):\n",
    "            title = [CNN_RSS.entries[i].title for i in range(len(CNN_RSS.entries))]\n",
    "            subtitle = [ getSubtitle(i) for i in range(len(CNN_RSS.entries))]\n",
    "            url = [CNN_RSS.entries[i].links[0].href for i in range(len(CNN_RSS.entries))]\n",
    "            timestamp = [getPublished(i) for i in range(len(CNN_RSS.entries))]\n",
    "            original_image = [getMedia(i) for i in range(len(CNN_RSS.entries))]\n",
    "            section = [list(CNN_URLS.keys())[k] for i in range(len(CNN_RSS.entries))]\n",
    "            CNN_RSS_feed_p = pd.DataFrame([title, subtitle, url, timestamp, original_image]).transpose()\n",
    "            CNN_feed.append(CNN_RSS_feed_p, ignore_index=True)\n",
    "    \n",
    "    return CNN_feed\n",
    "    \n",
    "def FOX_parser():\n",
    "    FOX_URLS = {'Fox':\"https://www.foxnews.com/about/rss\"}\n",
    "\n",
    "def WSJ_parser():\n",
    "    #Proxys?? Da error 503, hay que ver como hacerle\n",
    "    WSJ_URLS = {\n",
    "    \"World\": \"https://feeds.a.dj.com/rss/RSSWorldNews.xml\",\n",
    "    \"Opinion\": \"https://feeds.a.dj.com/rss/RSSOpinion.xml\",\n",
    "    \"Business\":\"https://feeds.a.dj.com/rss/WSJcomUSBusiness.xml\",\n",
    "    \"Markets\":\"https://feeds.a.dj.com/rss/RSSMarketsMain.xml\",\n",
    "    \"Technology\":\"https://feeds.a.dj.com/rss/RSSWSJD.xml\",\n",
    "    \"Lifestyle\":\"https://feeds.a.dj.com/rss/RSSLifestyle.xml\"}\n",
    "    \n",
    "def BBC_parser():\n",
    "    BBC_URLS = {\n",
    "    \"World\":\"http://feeds.bbci.co.uk/news/world/rss.xml\",\n",
    "    \"UK\":\"http://feeds.bbci.co.uk/news/uk/rss.xml\",\n",
    "    \"Business\":\"http://feeds.bbci.co.uk/news/business/rss.xml\",\n",
    "    \"Politics\": \"http://feeds.bbci.co.uk/news/politics/rss.xml\",\n",
    "    \"Health\": \"http://feeds.bbci.co.uk/news/health/rss.xml\",\n",
    "    \"Education\": \"http://feeds.bbci.co.uk/news/education/rss.xml\",\n",
    "    \"Science\": \"http://feeds.bbci.co.uk/news/science_and_environment/rss.xml\",\n",
    "    \"Technology\": \"http://feeds.bbci.co.uk/news/technology/rss.xml\",\n",
    "    \"Entretainment & Arts\": \"http://feeds.bbci.co.uk/news/entertainment_and_arts/rss.xml\"}\n",
    "\n",
    "    BBC_feed = pd.DataFrame()\n",
    "    for k in range(len(BBC_URLS)):\n",
    "        BBC_RSS = feedparser.parse(list(BBC_URLS.values())[k])\n",
    "        for i in range(len(BBC_RSS.entries)):\n",
    "            title = [BBC_RSS.entries[i].title for i in range(len(BBC_RSS.entries))]\n",
    "            subtitle = [ BBC_RSS.entries[i].summary for i in range(len(BBC_RSS.entries))]\n",
    "            url = [BBC_RSS.entries[i].links[0].href for i in range(len(BBC_RSS.entries))]\n",
    "            timestamp = [BBC_RSS.entries[0].published for i in range(len(BBC_RSS.entries))]\n",
    "            section = [list(BBC_RSS.keys())[k] for i in range(len(BBC_RSS.entries))]\n",
    "            BBC_RSS_feed_p = pd.DataFrame([title, subtitle, url, timestamp, section]).transpose()\n",
    "            BBC_feed.append(CNN_RSS_feed_p, ignore_index=True)\n",
    "    \n",
    "    return BBC_feed\n",
    "\n",
    "def USAT_parser():\n",
    "    \n",
    "    USAT_URLS = {\n",
    "    \"US\":\"http://rssfeeds.usatoday.com/UsatodaycomNation-TopStories\",\n",
    "    \"World\":\"http://rssfeeds.usatoday.com/UsatodaycomWorld-TopStories\",\n",
    "    \"Opinion\":\"http://rssfeeds.usatoday.com/News-Opinion\",\n",
    "    \"Sports\":\"http://rssfeeds.usatoday.com/UsatodaycomSports-TopStories\",\n",
    "    \"Lifestyle\":\"http://rssfeeds.usatoday.com/usatoday-LifeTopStories\",\n",
    "    \"Money\":\"http://rssfeeds.usatoday.com/UsatodaycomMoney-TopStories\",\n",
    "    \"Tech\":\"http://rssfeeds.usatoday.com/usatoday-TechTopStories\",\n",
    "    \"Travel\":\"http://rssfeeds.usatoday.com/UsatodaycomTravel-TopStories\"}\n",
    "    \n",
    "    def getUSATMedia(i):\n",
    "        try: \n",
    "            return USAT_feed_p.entries[i].links[1].href\n",
    "        except:\n",
    "            return \"N/A\"\n",
    "    USAT_feed = []\n",
    "    for k in range(len(USAT_URLS)):\n",
    "        USAT_RSS = feedparser.parse(list(USAT_URLS.values())[k]).entries\n",
    "        for i in range(len(USAT_RSS)):\n",
    "            title =  [USAT_feed_p.entries[i].title for i in range(len(USAT_RSS))]\n",
    "            #summary = [re.findall('(?<=<p>).*(?=</p>)', USAT_feed_p.entries[i].content[0].value)[0]) for i in range(len(USAT_RSS))]\n",
    "            url = [USAT_feed_p.entries[i].feedburner_origlink for i in range(len(USAT_RSS))]\n",
    "            timestamp = [USAT_feed_p.entries[i].published for i in range(len(USAT_RSS))]\n",
    "            image = [getUSATMedia(i) for i in range(len(USAT_RSS))]\n",
    "            section = [list(USAT_URLS.keys())[k] for i in range(len(USAT_RSS))]\n",
    "            USAT_feed.append([title, summary, url, timestamp, image, section])\n",
    "    \n",
    "    return USAT_feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_RSS = feedparser.parse(\"https://rss.nytimes.com/services/xml/rss/nyt/World.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_RSS.entries[0].media_content[0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_RSS.entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_feed = []\n",
    "def getNYTMedia(i):\n",
    "    try: \n",
    "        return media == NYT_RSS.entries[i].media_content[0]['url']\n",
    "    except:\n",
    "        return \"N/A\"\n",
    "    \n",
    "for k in range(len(NYT_URLS)):\n",
    "    NYT_RSS = feedparser.parse(list(NYT_URLS.values())[k]).entries\n",
    "    for i in range(len(NYT_RSS)):\n",
    "        title = [NYT_RSS.entries[i].title for i in range(len(NYT_RSS))]\n",
    "        url = [NYT_RSS.entries[i].link for i in range(len(NYT_RSS))]\n",
    "        summary = [NYT_RSS.entries[i].summary for i in range(len(NYT_RSS))]\n",
    "        timestamp = [NYT_RSS.entries[i].published for i in range(len(NYT_RSS))]\n",
    "        media = [getNYTMedia(i) for i in range(len(NYT_RSS))]\n",
    "        section = [list(NYT_URLS.keys())[k] for i in range(len(NYT_RSS))]\n",
    "        NYT_feed.append([title, summary, url, timestamp, media, section])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_URLS = {\n",
    "    \"World\":\"https://rss.nytimes.com/services/xml/rss/nyt/World.xml\",\n",
    "    \"Africa\":\n",
    "    \"Americas\":\n",
    "    \"Asia Pacific\":\n",
    "    \"Europe\":\n",
    "    \"Middle East\":\n",
    "    \"US\":\n",
    "    \"Education\":\n",
    "    \"Politics\":\"https://rss.nytimes.com/services/xml/rss/nyt/Politics.xml\"\n",
    "    \"Business\":\"https://rss.nytimes.com/services/xml/rss/nyt/Business.xml\",\n",
    "    \"Technology\":\"https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml\",\n",
    "    \"Sports\":\"https://rss.nytimes.com/services/xml/rss/nyt/Sports.xml\",\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'una'\n",
    "b='dos'\n",
    "c='tres'\n",
    "d = [a,b,c]\n",
    "unir = '|'.join(d)\n",
    "unir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
